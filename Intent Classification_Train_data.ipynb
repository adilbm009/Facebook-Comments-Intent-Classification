{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "589b0af6",
   "metadata": {
    "id": "3b1b32c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from numpy import float32 as REAL\n",
    "import pickle\n",
    "from gensim import utils\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from scipy.spatial.distance import cdist as scipy_cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "import tensorflow_hub as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "## GUSE Embeddings ##\n",
    "guse_embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29ab3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesing function\n",
    "stop_words_list= [\"a\", \"the\" , \"to\" , \"be\" ,\"is\" , \"an\", \"hi\", \"hello\", \"good morning\",\"goodmorning\",'goodevening',\n",
    "                  'good evening', \"hey\", \"wow\", \"buyer\", \"seller\", \"trader\", 'good', 'noon','morning','evening', 'here',\n",
    "                  \"here recordingenjoy\", \"all\", \"everyone\", \"any\", \"help\", \"pls\", \"please\", \"certainely\", \"fee\",\"like\" ,\"comment\", \"comment\",\n",
    "                 \"friends\",\"friend\", \"wi\", \"only\", \"verify\", \"asap\", \"some\", \"me\", \"fuck\", \"lol\", \"anyone\", \"cmnt\", \"omg\", \"wow\",\"wtf\", \"oo\",\n",
    "                 \"everyone\", \"page\", \"huh\", \"any\", \"new\", \"have\", \"my\", \"u\", \"attend\", \"look\", \"better\", \"interest\"]\n",
    "\n",
    "def lemmatize_word(text): \n",
    "    from nltk.tokenize import word_tokenize \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text) \n",
    "    lemmas = [lemmatizer.lemmatize(word,pos='v') for word in word_tokens] \n",
    "    return lemmas\n",
    "\n",
    "def clean_text(text,stop_words=None):\n",
    "    text = text.lower().strip() # Convert text to lower case and strip leading/trailing white spaces\n",
    "    text = text.lower().strip() # Convert text to lower case and strip leading/trailing white spaces\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove websites\n",
    "    text = re.sub(r'[^\\w\\s$]', '', text) # Remove punctuation except $\n",
    "    text= re.sub(r\"cant\",'can not',text)\n",
    "    text= re.sub(r\"didnt\",\"did not\",text)\n",
    "    text= re.sub(r\"dont\",\"do not\",text)\n",
    "    text= re.sub(r\"wasnt\",\"was not\",text)\n",
    "    text= re.sub(r\"wont\",\"will not\",text)    \n",
    "    text= re.sub('\\S+@\\S+','',text) #removing emailadderess\n",
    "    text= re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text) # removing all urls\n",
    "    text= re.sub(r\"\\[.*?\\]\", \" \",text)# removing data in square brackets\n",
    "    text= re.sub(\"[^a-z\\s$]+\",\" \",text)\n",
    "    text= re.sub(\"thank you|thank you soo much|thank you so much|thanks a lot|thank you very much\",\"thanks\",text)\n",
    "    text= re.sub(r\"(\\.|,|!)([a-zA-Z0-9]{1})\",r\"\\1 \\2\",text)\n",
    "    text= re.sub(' +',' ',text)  # removing extra spaces\n",
    "    text= text.replace(\",\", \"\")\n",
    "    text= text.replace(\".\", \"\")\n",
    "    text= text.replace(\"?\", \"\")\n",
    "    text= text.replace(\"\\n\",\" \")\n",
    "    text= text.replace(\"'pm\",\" contact\")# expansion contact\n",
    "    text= text.replace(\"'call me\",\" contact\")# expansion contact\n",
    "    text= text.replace(\"'m\",\" am\")# expansion am\n",
    "    text= text.replace(\"'ll\",\" will\")# expansion will\n",
    "    text= text.replace(\"'d\",\" would\")# expansion would\n",
    "    text= text.replace(\"n't\",\" not\")# expansion not\n",
    "    text= text.replace(\"'ve\",\" have\")# expansion have\n",
    "    text= text.replace(\"'s\",\" is\")# expansion is\n",
    "    text= text.replace(\"'re\",\" are\")# expansion are\n",
    "    text= text.replace(\"'prize\",\" price\")# expansion am\n",
    "    text= text.replace(\"ain't\",\"are not\")\n",
    "    text= text.replace(\"aint\",\"are not\")\n",
    "    text= text.replace(\"cause\",\"because\")\n",
    "    text= text.replace(\"couldn't\",\"could not\")\n",
    "    text= text.replace(\"couldnt\",\"could not\")\n",
    "    text= text.replace(\"didn't\",\"did not\")\n",
    "    text= text.replace(\"didnt\",\"did not\")\n",
    "    text= text.replace(\"doesn't\",\"does not\")\n",
    "    text= text.replace(\"doesnt\",\"does not\")\n",
    "    text= text.replace(\"don't\",\"do not\")\n",
    "    text= text.replace(\"dont\",\"do not\")\n",
    "    text= text.replace(\"everything's\",\"everything is\")\n",
    "    text= text.replace(\"everythings\",\"everything is\")\n",
    "    text= text.replace(\"everyones\",\"everyone is\")\n",
    "    text= text.replace(\"everyone's\",\"everyone is\")\n",
    "    text= text.replace(\"haven't\",\"have not\")\n",
    "    text= text.replace(\"havent\",\"have not\")\n",
    "    text= text.replace(\"hasn't\",\"has not\")\n",
    "    text= text.replace(\"hasnt\",\"has not\")\n",
    "    text= text.replace(\"hadn't\",\"had not\")\n",
    "    text= text.replace(\"hadnt\",\"had not\")\n",
    "    text= text.replace(\"he's\",\"he is\")\n",
    "    text= text.replace(\"hes\",\"he is\")\n",
    "    text= text.replace(\"i'll\",\"i will\")\n",
    "    text= text.replace(\"ill\",\"i will\")\n",
    "    text= text.replace(\"i'm\",\"i am\")\n",
    "    text= text.replace(\"im\",\"i am\")\n",
    "    text= text.replace(\"it's\",\"it is\")\n",
    "    text= text.replace(\"its\",\"it is\")\n",
    "    text= text.replace(\"i've\",\"i have\")\n",
    "    text= text.replace(\"ive\",\"i have\")\n",
    "    text= text.replace(\"let's\",\"let us\")\n",
    "    text= text.replace(\"lets\",\"let us\")\n",
    "    text= text.replace(\"she's\",\"she is\")\n",
    "    text= text.replace(\"shes\",\"she is\")\n",
    "    text= text.replace(\"shouldn't\",\"should not\")\n",
    "    text= text.replace(\"shouldnt\",\"should not\")\n",
    "    text= text.replace(\"that's\",\"that is\")\n",
    "    text= text.replace(\"thats\",\"that is\")\n",
    "    text= text.replace(\"there's\",\"there is\")\n",
    "    text= text.replace(\"theres\",\"there is\")\n",
    "    text= text.replace(\"they're\",\"they are\")\n",
    "    text= text.replace(\"theyre\",\"they are\")\n",
    "    text= text.replace(\"they've\",\"they have\")\n",
    "    text= text.replace(\"theyve\",\"they have\")\n",
    "    text= text.replace(\"wasn't\",\"was not\")\n",
    "    text= text.replace(\"wasnt\",\"was not\")\n",
    "    text= text.replace(\"we'd\",\"we would\")\n",
    "    text= text.replace(\"wed\",\"we would\")\n",
    "    text= text.replace(\"we'll\",\"we will\")\n",
    "    text= text.replace(\"well\",\"we will\")\n",
    "    text= text.replace(\"we're\",\"we are\")\n",
    "    text= text.replace(\"were\",\"we are\")\n",
    "    text= text.replace(\"weren't\", \"were not\")\n",
    "    text= text.replace(\"werent\", \"were not\")\n",
    "    text= text.replace(\"what's\", \"what is\")\n",
    "    text= text.replace(\"whats\", \"what is\")\n",
    "    text= text.replace(\"who's\", \"who is\")\n",
    "    text= text.replace(\"whos\", \"who is\")\n",
    "    text= text.replace(\"won't\", \"will not\")\n",
    "    text= text.replace(\"wont\", \"will not\")\n",
    "    text= text.replace(\"wouldn't\", \"would not\")\n",
    "    text= text.replace(\"wouldnt\", \"would not\")\n",
    "    text= text.replace(\"you'll\", \"you will\")\n",
    "    text= text.replace(\"youll\", \"you will\")\n",
    "    text= text.replace(\"you're\", \"you are\")\n",
    "    text= text.replace(\"youre\", \"you are\")\n",
    "    text= text.replace(\"you've\", \"you have\")\n",
    "    text= text.replace(\"youve\", \"you have\")\n",
    "    text= text.replace(\"kiss\", \"case\")\n",
    "    text= text.replace(\"gonna\", \"going to\")\n",
    "    text= text.replace(\"yeah\", \"yes\")\n",
    "    text= text.replace(\"may\", \"my\")\n",
    "    text= text.replace(\"ise\", \"is\")\n",
    "    text= text.strip()\n",
    "    text= text.replace('uhhuh', '')\n",
    "    text= lemmatize_word(text)\n",
    "    if stop_words is None:\n",
    "        stop_words= stop_words_list\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text\n",
    "def remove_emoji(text):\n",
    "    # Regular expression to match emoji\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6f2a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading files\n",
    "df = pd.read_csv(r'out.csv')\n",
    "kpi_df = pd.read_excel(r'kpi_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d998cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying preprocessing\n",
    "df = df.drop_duplicates(subset = ['FeedText'],keep = 'last').reset_index(drop = True)\n",
    "df=df[~(df.FeedText=='')]\n",
    "df.dropna(subset=['FeedText'], inplace=True)\n",
    "df['FeedText'] = df['FeedText'].apply(remove_emoji)\n",
    "df['FeedText'] = df['FeedText'].str.replace('\\d+', '')\n",
    "df['FeedText'] = df.FeedText.dropna().apply(clean_text).map(lambda x: ' '.join(i for i in x))\n",
    "df=df[~(df.FeedText=='')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "316a292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faiss Training and Simiarity Search Function\n",
    "def faiss_index_creation(probing_baselines):\n",
    "    dim = 512\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(guse_embed((probing_baselines)).numpy())\n",
    "    return index\n",
    "\n",
    "def search_faiss(df,probing_baselines,faiss_index,faiss_similarity_threshold):\n",
    "    li_res_guss = []\n",
    "    query= df.FeedText.tolist()\n",
    "    dist, idx = faiss_index.search((guse_embed(query).numpy()),2)\n",
    "    for i in range(len(query)):\n",
    "        if np.max(dist[i]) >= faiss_similarity_threshold:\n",
    "            li_res_guss.append((query[i], idx[i][np.argmax(dist[i])], np.max(dist[i])))\n",
    "        else:\n",
    "            li_res_guss.append((query[i], None, 0))\n",
    "    return li_res_guss\n",
    "############################### Main Function For Checking Intent #########################################\n",
    "def check_intent(df, kpi_df, kpi_type):\n",
    "    #Provide threshold for different baselines\n",
    "    faiss_similarity_threshold_buyer_seller = 0.448\n",
    "    faiss_similarity_threshold_buyer= 0.447\n",
    "    faiss_similarity_threshold_seller = 0.448\n",
    "    #Loading the baselines for checking buyer, seller, buyer_seller\n",
    "    d=defaultdict()\n",
    "    kpi_df=kpi_df[kpi_df.sub_type==kpi_type]\n",
    "    d['kpi_id']=kpi_df.id.iloc[0]\n",
    "    d['kpi_name']=kpi_df.name.iloc[0]\n",
    "    actor_type=kpi_df.actor_type.iloc[0]\n",
    "    all_baselines=ast.literal_eval(kpi_df.processed_baseline.iloc[0])\n",
    "    buyer_seller_baselines=all_baselines['buyer_seller']\n",
    "    buyer_baselines=all_baselines['buyer_baselines']\n",
    "    seller_baselines=all_baselines['seller_baselines']\n",
    "    \n",
    "    res=[]\n",
    "    id = df['Id'].unique()[0]\n",
    "    query=df.FeedText.tolist()\n",
    "    if any(df.ProfileName.isna()):\n",
    "        user_name=\"Name-Unavailable\"\n",
    "    else:\n",
    "        try:\n",
    "            user_name =  df['ProfileName'].unique()[0]\n",
    "        except IndexError:\n",
    "            user_name='None'\n",
    "    #Creating indexes for similarity search\n",
    "    index_buyer_seller=faiss_index_creation(buyer_seller_baselines)\n",
    "    index_buyer=faiss_index_creation(buyer_baselines)\n",
    "    index_seller=faiss_index_creation(seller_baselines)\n",
    "    \n",
    "    #Similarity Search for three baselines\n",
    "    similarity_results_buyer_seller = search_faiss(df,buyer_seller_baselines,index_buyer_seller,faiss_similarity_threshold_buyer_seller)\n",
    "    check_met_similarity_results_buyer_seller=[sm[2] for sm in similarity_results_buyer_seller if  sm[2]>0]\n",
    "    \n",
    "    similarity_results_buyer = search_faiss(df,buyer_baselines,index_buyer,faiss_similarity_threshold_buyer)\n",
    "    check_met_similarity_results_buyer=[sm[2] for sm in similarity_results_buyer if  sm[2]>0]\n",
    "\n",
    "    similarity_results_seller = search_faiss(df,seller_baselines,index_seller,faiss_similarity_threshold = faiss_similarity_threshold_seller)\n",
    "    check_met_similarity_results_seller=[sm[2] for sm in similarity_results_seller if  sm[2]>0]\n",
    "    \n",
    "    #Main conditions check for intent classifications\n",
    "    if len(check_met_similarity_results_buyer_seller)> 0:\n",
    "        res.append((id, user_name, 'buyer_seller'))\n",
    "    elif len(check_met_similarity_results_buyer) > 0:\n",
    "        res.append((id, user_name, 'buyer'))\n",
    "    elif len(check_met_similarity_results_seller) > 0:\n",
    "        res.append((id, user_name, 'seller'))\n",
    "    else:\n",
    "        res.append((id, user_name, 'neutral'))\n",
    "    #Storing the results\n",
    "    d['score'] =  res\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5bc9f3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 45min 47s\n",
      "Wall time: 54min 39s\n"
     ]
    }
   ],
   "source": [
    "#Running the function on the whole dataframe\n",
    "%%time\n",
    "result = df.groupby('Id').apply(lambda x: check_intent(x, kpi_df , kpi_type=\"check_intent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "558ea607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the results to dataframe\n",
    "resultdf= pd.DataFrame(result)\n",
    "resultdf.reset_index(inplace=True)\n",
    "resultdf['user_name'] = [i['score'][0][1] if len(i['score'])>0 else 'NA' for i in resultdf[0]]\n",
    "resultdf['intent'] = [i['score'][0][2] if len(i['score'])>0 else 'NA' for i in resultdf[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79c8edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "      <th>user_name</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5b1aa7ee8bb55b300895a774</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Shawn Lemoine</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b1aa7ee8bb55b300895a776</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Shawn Lemoine</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5b1aa7ee8bb55b300895a778</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Shawn Lemoine</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5b1aa7ee8bb55b300895a77a</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Gian Fortuin</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5b1aa7ee8bb55b300895a77c</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Shawn Lemoine</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134853</th>\n",
       "      <td>5d11b492a3df6323f881ab73</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Marwa Mohamed</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134854</th>\n",
       "      <td>5d11b7e7a3df6323f881ab77</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Oun Sida</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134855</th>\n",
       "      <td>5d11b7e7a3df6323f881ab7b</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Jonathan Reeves</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134856</th>\n",
       "      <td>5d11b7e7a3df6323f881ab7f</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>Michael Dass Sumaylo</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134857</th>\n",
       "      <td>5d11b7e7a3df6323f881ab81</td>\n",
       "      <td>{'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...</td>\n",
       "      <td>William Ugwu</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134858 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Id  \\\n",
       "0       5b1aa7ee8bb55b300895a774   \n",
       "1       5b1aa7ee8bb55b300895a776   \n",
       "2       5b1aa7ee8bb55b300895a778   \n",
       "3       5b1aa7ee8bb55b300895a77a   \n",
       "4       5b1aa7ee8bb55b300895a77c   \n",
       "...                          ...   \n",
       "134853  5d11b492a3df6323f881ab73   \n",
       "134854  5d11b7e7a3df6323f881ab77   \n",
       "134855  5d11b7e7a3df6323f881ab7b   \n",
       "134856  5d11b7e7a3df6323f881ab7f   \n",
       "134857  5d11b7e7a3df6323f881ab81   \n",
       "\n",
       "                                                        0  \\\n",
       "0       {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "1       {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "2       {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "3       {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "4       {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "...                                                   ...   \n",
       "134853  {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "134854  {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "134855  {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "134856  {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "134857  {'kpi_id': 1, 'kpi_name': 'CHECK_INTENT', 'sco...   \n",
       "\n",
       "                   user_name   intent  \n",
       "0              Shawn Lemoine  neutral  \n",
       "1              Shawn Lemoine  neutral  \n",
       "2              Shawn Lemoine  neutral  \n",
       "3               Gian Fortuin  neutral  \n",
       "4              Shawn Lemoine  neutral  \n",
       "...                      ...      ...  \n",
       "134853         Marwa Mohamed  neutral  \n",
       "134854              Oun Sida  neutral  \n",
       "134855       Jonathan Reeves  neutral  \n",
       "134856  Michael Dass Sumaylo  neutral  \n",
       "134857          William Ugwu  neutral  \n",
       "\n",
       "[134858 rows x 4 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f467d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral         134453\n",
       "buyer              251\n",
       "buyer_seller       105\n",
       "seller              49\n",
       "Name: intent, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdf['intent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2ae709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultdf.to_excel('resultdf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e08998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of your functions and variables\n",
    "model = {'lemmatize_word': lemmatize_word,\n",
    "         'clean_text': clean_text,\n",
    "         'stop_words_list': stop_words_list,\n",
    "         'faiss_index_creation' : faiss_index_creation,\n",
    "         'search_faiss' : search_faiss,\n",
    "         'check_intent' : check_intent}\n",
    "\n",
    "# save the model as a pkl file\n",
    "with open('my_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67defa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load the model from the pkl file\n",
    "with open('my_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2416ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919781d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
